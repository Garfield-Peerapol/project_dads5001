import streamlit as st
from collections import Counter
import pandas as pd
from pymongo import MongoClient
import json

#for word clound
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus import thai_stopwords
from pythainlp.corpus import common
from pythainlp.util import Trie
import re

st.title("YouTube Comments Analysis")
st.markdown("""
### üî∞ Introduction
This project is an extension of the DADS5002 course, focusing on text classification using machine learning techniques. The objective is to automatically categorize comments extracted from YouTube videos into predefined classifications, helping to uncover insights from user feedback.
"""
"""### üîç Data Source and Collection
Text Comments: Collected from YouTube comment sections of car review videos.
Target Brand: BYD, specifically its top 3 best-selling models in the Thai market ‚Äî Atto 3, Seal, and Dolphin.
YouTube Channel: Data was gathered from the autolifethailand official channel, which has over 1.06 million subscribers and is known for its trusted automotive content.
The dataset consists of comments from three BYD-related car review clips, which serve as the foundation for building and evaluating the classification model.
"""
"""### üí° AI-Powered Insight Assistant
To enhance the value of this project, an AI-powered assistant feature is integrated for premium users, enabling interactive exploration of classified comments. This assistant helps identify key highlights, detect emerging themes, and provide contextual insight summaries, supporting faster and smarter decision-making based on public sentiment.
""")

# === YouTube Video IDs ===
video_ids = ["OMV9F9zB4KU", "87lJCDADWCo", "CbkX7H-0BIU"]
# === Show Videos ===
st.subheader("‚ñ∂Ô∏è Video Reference üî¥")
col1, col2, col3 = st.columns(3)
with col1:
    st.video(f"https://www.youtube.com/watch?v={video_ids[0]}")
    st.caption("BYD Atto3")
with col2:
    st.video(f"https://www.youtube.com/watch?v={video_ids[1]}")
    st.caption("BYD Seal")
with col3:
    st.video(f"https://www.youtube.com/watch?v={video_ids[2]}")
    st.caption("BYD Dolphin")
# === Sidebar: Conversation History ===

# === MongoDB ===
mongo_uri = "mongodb+srv://readwrite:OSbtDM3XE8nP2JqT@voranitha.z6voe4w.mongodb.net/"
 
@st.cache_resource
def get_database():
    """
    ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö MongoDB Atlas ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô object ‡∏Ç‡∏≠‡∏á database
    ‡πÉ‡∏ä‡πâ st.cache_resource ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
    """
    try:
        client = MongoClient(mongo_uri)
        client.admin.command('ping') # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
        return client.car # ‡∏Ñ‡∏∑‡∏ô database 'car'
    except ConnectionFailure as e:
        st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö MongoDB ‡πÑ‡∏î‡πâ: {e}")
        st.stop() # ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Streamlit ‡∏´‡∏≤‡∏Å‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
    except OperationFailure as e:
        st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô MongoDB: {e}")
        st.stop()
    except Exception as e:
        st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î: {e}")
        st.stop()
 
# --- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Streamlit App ---
 
# ‡∏î‡∏∂‡∏á database object
db = get_database()
collection = db.comment
data = list(collection.find())  # Get all documents as a list of dicts
df = pd.DataFrame(data)         # Convert to DataFrame

# ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞ video_title
#comments = list(collection.find({}, {"video_title": 1}))
#comments = list(collection.find({}, {"_id": 0, "video_title": 1}))

# ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô DataFrame
#df = pd.DataFrame(comments)
#st.write(comments[:5]) 

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
if 'video_title' not in df.columns:
    st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'video_title'")
else:
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏ï‡πà‡∏≠ video_title
    video_counts = df['video_title'].value_counts().reset_index()
    video_counts.columns = ['video_title', 'count']

    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á label ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    def generate_label(title):
        title_upper = title.upper()
        if "ATTO" in title_upper:
            return "BYD Atto3"
        elif "SEAL" in title_upper:
            return "BYD Seal"
        elif "DOLPHIN" in title_upper:
            return "BYD Dolphin"
        else:
            return "‡∏≠‡∏∑‡πà‡∏ô ‡πÜ"

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå label
    video_counts['Model'] = video_counts['video_title'].apply(generate_label)

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà
    result_df = video_counts[['Model', 'video_title', 'count']]

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    st.subheader("üìä Video Comment Counts")
    st.dataframe(result_df)

# Word Cloud
comments_collection = db.comment

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Dictionary ---
# ‡πÇ‡∏´‡∏•‡∏î‡∏û‡∏à‡∏ô‡∏≤‡∏ô‡∏∏‡∏Å‡∏£‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á pythainlp
default_words = common.thai_words()
custom_word_list = list(default_words) + ["‡πÑ‡∏ü‡∏ü‡πâ‡∏≤", "‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡πÑ‡∏ü‡∏ü‡πâ‡∏≤", "‡πÅ‡∏ö‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡∏µ‡πà", "‡∏Ç‡∏±‡∏ö‡∏Ç‡∏µ‡πà","‡∏ã‡∏∑‡πâ‡∏≠","‡πÄ‡∏á‡∏¥‡∏ô","‡πÄ‡∏™‡∏µ‡∏¢‡∏á","‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà","‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô","‡∏ä‡∏≤‡∏£‡πå‡∏à","‡∏Å‡∏•‡πâ‡∏≠‡∏á","‡πÄ‡∏ó‡∏Ñ","‡∏≠‡∏≠‡∏ü‡∏ä‡∏±‡πà‡∏ô","‡∏™‡∏µ","‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô","‡∏´‡∏ô‡πâ‡∏≤","‡∏î‡∏µ‡πÑ‡∏ã‡∏ô‡πå"] 
custom_dictionary = Trie(custom_word_list)
# ----------------------------------

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Word Cloud ---
# --- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Mapping ‡∏ä‡∏∑‡πà‡∏≠ Model ‡∏Å‡∏±‡∏ö Keyword ‡πÉ‡∏ô title ---
model_keywords = {
    "BYD Atto3": "ATTO",
    "BYD Dolphin": "DOLPHIN",
    "BYD Seal": "SEAL"
}

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Dropdown ‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model ---
selected_model = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏∏‡πà‡∏ô‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π Word Cloud", list(model_keywords.keys()))

if selected_model and st.button("‡∏™‡∏£‡πâ‡∏≤‡∏á Word Cloud"):
    try:
        keyword = model_keywords[selected_model]

        # --- ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£ Query ‡∏à‡∏≤‡∏Å MongoDB ‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ---
        # Query ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏ó‡∏µ‡πà title ‡∏°‡∏µ‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡∏Ç‡∏≠‡∏á‡∏£‡∏∏‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        cursor = comments_collection.find(
            {"video_title": {"$regex": keyword, "$options": "i"}},
            {"comment": 1} # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ field 'comment' ‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£
        )
        # ---------------------------------------------------

        # ‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        # re.sub(r"[^\u0E00-\u0E7F\s]+", " ", doc.get("comment", "")) 
        #   - [^\u0E00-\u0E7F\s]+: ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÉ‡∏î‡πÜ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢ (\u0E00-\u0E7F) 
        #     ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ß‡∏£‡∏£‡∏Ñ‡∏ß‡πà‡∏≤‡∏á (\s) ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ (+)
        #   - " ": ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ß‡πà‡∏≤‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö
        comments = [re.sub(r"[^\u0E00-\u0E7F\s]+", " ", doc.get("comment", "")) for doc in cursor]
        full_text = " ".join(comments)

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not full_text.strip(): # ‡πÉ‡∏ä‡πâ .strip() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏ß‡∏£‡∏£‡∏Ñ‡∏ß‡πà‡∏≤‡∏á
            st.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∏‡πà‡∏ô **{selected_model}** ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î **'{keyword}'**")
            st.stop() # ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î error ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

        # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥
        tokens = word_tokenize(full_text, engine="newmm")

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Stopwords ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
        # ‡∏£‡∏ß‡∏° stopwords ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÜ
        combined_stopwords = set(thai_stopwords()).union({
            "‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏Ñ‡πà‡∏∞", "‡πÜ", "‡∏ô‡∏∞", "‡πÄ‡∏•‡∏¢", "‡πÄ‡∏õ‡πá‡∏ô", "‡∏Ñ‡∏∑‡∏≠", "‡∏ß‡πà‡∏≤", "‡πÑ‡∏î‡πâ", "‡∏à‡∏∞", "‡∏Å‡πá", "‡πÅ‡∏•‡πâ‡∏ß", 
            "‡∏°‡∏≤‡∏Å", "‡∏ô‡πâ‡∏≠‡∏¢", "‡πÑ‡∏õ", "‡∏°‡∏≤", "‡∏≠‡∏¢‡∏π‡πà", "‡∏°‡∏µ", "‡πÑ‡∏°‡πà", "‡∏Ñ‡∏ô", "‡∏Ñ‡∏±‡∏ô", "‡∏ï‡∏±‡∏ß", "‡∏£‡∏∏‡πà‡∏ô", "‡∏ó‡∏≥‡πÑ‡∏°", "‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ", "‡πÄ‡∏´‡πá‡∏ô", "‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô", "‡πÉ‡∏ô", "‡∏£‡∏π‡∏õ"
        })

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Allowed Words (‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
        allowed_words = {
            "‡∏î‡∏µ", "‡πÅ‡∏£‡∏á", "‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î", "‡∏™‡∏ß‡∏¢", "‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å", "‡∏£‡∏≤‡∏Ñ‡∏≤", "‡∏Ñ‡∏∏‡πâ‡∏°", "‡∏™‡∏ö‡∏≤‡∏¢", "‡πÄ‡∏£‡πá‡∏ß", "‡πÄ‡∏á‡∏µ‡∏¢‡∏ö",
            "‡∏ã‡∏∑‡πâ‡∏≠", "‡∏Ç‡∏≤‡∏¢", "‡∏ñ‡∏π‡∏Å", "‡πÅ‡∏û‡∏á", "‡∏•‡∏î", "‡∏ö‡∏≤‡∏ó", "‡∏•‡πâ‡∏≤‡∏ô", "‡πÅ‡∏™‡∏ô", "‡πÄ‡∏á‡∏¥‡∏ô", "‡πÄ‡∏™‡∏µ‡∏¢‡∏á",
            "‡∏Ç‡∏±‡∏ö‡∏Ç‡∏µ‡πà", "‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô", "‡∏ñ‡∏µ‡πà", "‡∏ó‡∏ô", "‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û", "‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà", "‡∏õ‡∏•‡∏≠‡∏î", "‡∏£‡∏∞‡∏ö‡∏ö", 
            "‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô", "‡∏ä‡∏≤‡∏£‡πå‡∏à", "‡∏Å‡∏•‡πâ‡∏≠‡∏á", "‡πÅ‡∏ö‡∏ï", "‡∏ä‡πà‡∏ß‡∏¢", "‡πÄ‡∏ó‡∏Ñ", "‡∏≠‡∏≠‡∏ü‡∏ä‡∏±‡πà‡∏ô", "‡πÑ‡∏ü‡∏ü‡πâ‡∏≤",
            "‡∏™‡∏µ", "‡∏†‡∏≤‡∏¢", "‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô", "‡∏ó‡∏£‡∏á", "‡∏£‡∏π‡∏õ", "‡∏´‡∏ô‡πâ‡∏≤", "‡∏´‡∏£‡∏π", "‡∏´‡∏£‡∏≤", "‡∏ô‡∏≠‡∏Å", "‡∏á‡∏≤‡∏°", "‡∏î‡∏µ‡πÑ‡∏ã‡∏ô‡πå"
        }
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥:
        # 1. ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Stopwords 
        # 2. ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 2 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏≠‡∏≠‡∏Å (‡πÄ‡∏ä‡πà‡∏ô ‡∏û‡∏¢‡∏≤‡∏á‡∏Ñ‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
        # 3. ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô allowed_words ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        filtered_tokens = [
            w for w in tokens 
            if w not in combined_stopwords and len(w) > 1 and w in allowed_words
        ]

        # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
        if not filtered_tokens:
            st.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∏‡πà‡∏ô **{selected_model}**")
            st.stop()

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Word Cloud
        wordcloud = WordCloud(
            font_path="fonts/THSarabunNew.ttf",
            width=800,
            height=400,
            background_color="white",
            collocations=False # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        ).generate(" ".join(filtered_tokens)) # ‡πÉ‡∏ä‡πâ filtered_tokens ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
        fig, ax = plt.subplots()
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis("off")
        st.pyplot(fig)
        st.success(f"‡πÅ‡∏™‡∏î‡∏á Word Cloud ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: **{selected_model}**")

    except Exception as e:
        st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Word Cloud: {e}")
