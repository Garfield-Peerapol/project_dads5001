import streamlit as st

st.title("üß† Comment Classification")

# --- Sub-topic Navigation ---
option = st.radio("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢", ["üîç Preview Comments", "üß™ ML Modeling"])

# --- Section 1 ---
if option == "üîç Preview Comments":
    st.subheader("üîç ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå")
    # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏à‡∏≤‡∏Å MongoDB ‡∏´‡∏£‡∏∑‡∏≠ CSV ‡πÅ‡∏•‡πâ‡∏ß‡πÇ‡∏ä‡∏ß‡πå
    st.write("‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå top 5 ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î")

# --- Section 2 ---
elif option == "üß™ ML Modeling":
    st.subheader("üß™ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning")
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model (Random Forest, Neural Network, etc.)
    model_type = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•", ["Random Forest", "Neural Network"])
    st.write(f"‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•: {model_type}")
    # ‡πÇ‡∏´‡∏•‡∏î stopwords ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

    df = pd.read_csv(uploaded_file)
    #st.success("‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
    #st.write("üìå ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
    st.dataframe(df.head(10))
    
    
    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    thai_stopwords_set = set(thai_stopwords())
    def remove_emojis_and_symbols(text):
        emoji_pattern = re.compile("[" 
            u"\U0001F600-\U0001F64F"
            u"\U0001F300-\U0001F5FF"
            u"\U0001F680-\U0001F6FF"
            u"\U0001F1E0-\U0001F1FF"
            u"\U00002700-\U000027BF"
            u"\U000024C2-\U0001F251"
            "]+", flags=re.UNICODE)
        text = emoji_pattern.sub(r'', text)
        text = re.sub(r"[^\u0E00-\u0E7Fa-zA-Z0-9\s]", "", text)
        return text
    
    def clean_text_combined(text):
        if isinstance(text, str):
            text = remove_emojis_and_symbols(text)
            tokens = word_tokenize(text, engine='newmm')
            seen = set()
            unique_tokens = [token for token in tokens if not (token in seen or seen.add(token))]
            filtered_tokens = [token for token in unique_tokens if token not in thai_stopwords_set and token.strip()]
            return " ".join(filtered_tokens)

    #select random forest
    if model_type=="Random Forest":
        def load_all_models():
            model = joblib.load('final_model_rf.pkl')
            vectorizer = joblib.load('vectorizer_rf.pkl')
            encoder = joblib.load('label_encoder_rf.pkl')
            selector = joblib.load('selector_rf.pkl')
            return model, vectorizer, encoder, selector
            
        model, vectorizer, encoder, selector = load_all_models()

        #processing and predict data
        cleaned_texts = [clean_text_combined(text) for text in df.iloc[:,0]]
        vect_texts = vectorizer.transform(cleaned_texts)
        selected_features = selector.transform(vect_texts)
        predictions = model.predict(selected_features)
        decoded_predictions = encoder.inverse_transform(predictions)

    #select Neural Network
    if model_type=="Neural Network":
        def load_all_models():
            model = joblib.load('final_model_NN.pkl')
            vectorizer = joblib.load('vectorizer_NN.pkl')
            encoder = joblib.load('label_encoder_NN.pkl')
            selector = joblib.load('selector_NN.pkl')
            return model, vectorizer, encoder, selector
            
        model, vectorizer, encoder, selector = load_all_models()

        #processing and predict data
        cleaned_new_texts = [clean_text_combined(text) for text in df.iloc[:,0]]
        vect_new_texts = vectorizer.transform(cleaned_new_texts)
        selected_features_new_texts = selector.transform(vect_new_texts)  # Apply selector
        new_predictions_probs = model.predict(selected_features_new_texts)
        new_predictions_classes = np.argmax(new_predictions_probs, axis=1)
        decoded_predictions = encoder.inverse_transform(new_predictions_classes)

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    results_df = pd.DataFrame({
        '‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö': df.iloc[:,0],
        '‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ': decoded_predictions
    })

    st.write("‚úÖ ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:")
    st.dataframe(results_df)
        




