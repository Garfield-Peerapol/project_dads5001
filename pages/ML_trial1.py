import streamlit as st
import pandas as pd
import numpy as np
import joblib
import re
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus.common import thai_stopwords
import matplotlib as mpl
import matplotlib.pyplot as plt
from keras.models import load_model
import time

#1 load original data
df = pd.read_csv('pages/predict_data.csv')

#2 def text preprocessing function
def remove_emojis_and_symbols(text):
  emoji_pattern = re.compile("[" 
            u"\U0001F600-\U0001F64F"
            u"\U0001F300-\U0001F5FF"
            u"\U0001F680-\U0001F6FF"
            u"\U0001F1E0-\U0001F1FF"
            u"\U00002700-\U000027BF"
            u"\U000024C2-\U0001F251"
            "]+", flags=re.UNICODE)
  text = emoji_pattern.sub(r'', text)
  text = re.sub(r"[^\u0E00-\u0E7Fa-zA-Z0-9\s]", "", text)
  return text
  
def clean_text_combined(text):
  if isinstance(text, str):
    text = remove_emojis_and_symbols(text)
    tokens = word_tokenize(text, engine='newmm')
    seen = set()
    unique_tokens = [token for token in tokens if not (token in seen or seen.add(token))]
    filtered_tokens = [token for token in unique_tokens if token not in thai_stopwords_set and token.strip()]
    return " ".join(filtered_tokens)
def thai_tokenizer(text):
  return word_tokenize(text, engine="newmm")


#3 processing for random forest
@st.cache_resource
model_rf = joblib.load('pages/final_model_rf.pkl')
vectorizer_rf = joblib.load('pages/vectorizer_rf.pkl')
encoder_rf = joblib.load('pages/label_encoder_rf.pkl')
selector_rf = joblib.load('pages/selector_rf.pkl')
vectorizer.tokenizer = thai_tokenizer

cleaned_texts = [clean_text_combined(text) for text in df.iloc[:,0]]
vect_texts = vectorizer.transform(cleaned_texts)
selected_features = selector.transform(vect_texts)
predictions = model.predict(selected_features)
decoded_predictions_rf = encoder.inverse_transform(predictions)

df_rf=pd.DataFrame({
        'Comment': df.iloc[:,0],
        'Predicted Label': decoded_predictions_rf
    })


# Neural Network
@st.cache_resource
model_nn = load_model('pages/final_model_NN.keras') 
vectorizer_nn = joblib.load('pages/vectorizer_NN.pkl')
encoder_nn = joblib.load('pages/label_encoder_NN.pkl')
selector_nn = joblib.load('pages/selector_NN.pkl')
vectorizer.tokenizer = thai_tokenizer

cleaned_new_texts = [clean_text_combined(text) for text in df.iloc[:,0]]
vect_new_texts = vectorizer.transform(cleaned_new_texts)
selected_features_new_texts = selector.transform(vect_new_texts)  # Apply selector
new_predictions_probs = model.predict(selected_features_new_texts)
new_predictions_classes = np.argmax(new_predictions_probs, axis=1)
decoded_predictions = encoder.inverse_transform(new_predictions_classes)

df_nn=pd.DataFrame({
        'Comment': df.iloc[:,0],
        'Predicted Label': decoded_predictions_rf
    })

st.dataframe(df_rf)
st.dataframe(df_nn)





#3
"""



st.title("üß† Comment Classification")
st.write("‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")



# --- Sub-topic Navigation ---
option = st.radio("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢", ["üîç Preview Comments", "üß™ ML Modeling"])

# --- Section 1 ---
if option == "üîç Preview Comments":
    st.subheader("üîç ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå")
    # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏à‡∏≤‡∏Å MongoDB ‡∏´‡∏£‡∏∑‡∏≠ CSV ‡πÅ‡∏•‡πâ‡∏ß‡πÇ‡∏ä‡∏ß‡πå
    st.write("‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå top 5 ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î")

# --- Section 2 ---
elif option == "üß™ ML Modeling":
    st.subheader("üß™ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning")
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model (Random Forest, Neural Network, etc.)
    model_type = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•", ["Random Forest", "Neural Network"])
    st.write(f"‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•: {model_type}")
    # ‡πÇ‡∏´‡∏•‡∏î stopwords ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

    #st.success("‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
    #st.write("üìå ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
    st.dataframe(df.head(10))
    
    
    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    thai_stopwords_set = set(thai_stopwords())

    


    #select random forest
    if model_type=="Random Forest":
        

        #processing and predict data

    
    
    #select Neural Network
    if model_type=="Neural Network":

            
        from pythainlp.tokenize import word_tokenize
        model, vectorizer, encoder, selector = load_all_models_NN()
        

        #processing and predict data

    
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    results_df = pd.DataFrame({
        '‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö': df.iloc[:,0],
        '‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ': decoded_predictions
    })

    st.write("‚úÖ ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:")
    st.dataframe(results_df)
    """    
